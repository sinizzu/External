# test 데이터 넣기

import weaviate
import os
import weaviate.classes.config as wc
from weaviate.classes.config import Configure, Property, DataType



# 환경 변수 가져오기
URL = os.getenv("WCS_URL")
APIKEY = os.getenv("WCS_API_KEY")
HUGGING = os.getenv("HUGGINGFACE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


headers = {
    "X-HuggingFace-Api-Key": HUGGING,
    "X-OpenAI-Api-Key" : OPENAI_API_KEY

}

client = weaviate.connect_to_wcs(
    cluster_url=URL,
    auth_credentials=weaviate.auth.AuthApiKey(APIKEY),
    headers=headers
    )


# 연결 확인
if client.is_ready():
    print("Weaviate Cloud에 성공적으로 연결되었습니다.")
else:
    print("Weaviate Cloud에 연결할 수 없습니다.")


texts = '''
arXiv : 1910.14296v2 [ cs.CL ] 6 Oct 2020\\n\\nLIMIT - BERT : Linguistics Informed Multi - Task BERT\\n\\n1,2,3\\n\\nJunru Zhou1,2,3 , Zhuosheng Zhang 1,2,3 , Hai Zhao1,2,3 * , Shuailiang Zhang 1 ¹Department of Computer Science and Engineering , Shanghai Jiao Tong University 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering , Shanghai Jiao Tong University , Shanghai , China 3 MOE Key Lab of Artificial Intelligence , AI Institute , Shanghai Jiao Tong University { zhoujunru , zhangzs}@sjtu.edu.cn , zhaohai@cs.sjtu.edu.cn\\n\\nAbstract\\n\\nIn this paper , we present Linguistics Informed Multi - Task BERT ( LIMIT - BERT ) for learning language representations across multiple lin- guistics tasks by Multi - Task Learning . LIMIT- BERT includes five key linguistics tasks : Part- Of - Speech ( POS ) tags , constituent and de- pendency syntactic parsing , span and depen- dency semantic role labeling ( SRL ) . Differ- ent from recent Multi - Task Deep Neural Net- works ( MT - DNN ) , our LIMIT - BERT is fully linguistics motivated and thus is capable of adopting an improved masked training objec- tive according to syntactic and semantic con- stituents . Besides , LIMIT - BERT takes a semi- supervised learning strategy to offer the same large amount of linguistics task data as that for the language model training . As a re- sult , LIMIT - BERT not only improves linguis- tics tasks performance , but also benefits from a regularization effect and linguistics infor- mation that leads to more general representa- tions to help adapt to new tasks and domains . LIMIT - BERT outperforms the strong baseline Whole Word Masking BERT on both depen- dency and constituent syntactic / semantic pars- ing , GLUE benchmark , and SNLI task . Our practice on the proposed LIMIT - BERT also en- ables us to release a well pre - trained model for multi - purpose of natural language processing tasks once for all .\\n\\n1 Introduction\\n\\n1 Introduction\\n\\nRecently , pre - trained language models have shown greatly effective across a range of linguistics in- spired natural language processing ( NLP ) tasks such as syntactic parsing , semantic parsing and\\n\\n* Corresponding author . This paper was partially sup- ported by National Key Research and Development Program of China ( No. 2017YFB0304100 ) , Key Projects of Na- tional Natural Science Foundation of China ( U1836222 and 61733011 ) , Huawei - SJTU long term AI project , Cutting - edge Machine reading comprehension and language model .\\n\\nso on ( Zhou and Zhao , 2019 ; Zhou et al . , 2020 ; Ouchi et al . , 2018 ; He et al . , 2018b ; Li et al . , 2019 ) , when taking the latter as downstream tasks for the former . In the meantime , introducing linguis- tic clues such as syntax and semantics into the pre - trained language models may furthermore en- hance other downstream tasks such as various Nat- ural Language Understanding ( NLU ) tasks ( Zhang et al . , 2020a , b ) . However , nearly all existing lan- guage models are usually trained on large amounts of unlabeled text data ( Peters et al . , 2018 ; Devlin et al . , 2019 ) , without explicitly exploiting linguis- tic knowledge . Such observations motivate us to jointly consider both types of tasks , pre - training language models , and solving linguistics inspired NLP problems . We argue such a treatment may benefit from two - fold . ( 1 ) Joint learning is a better way to let the former help the latter in a bidirec- tional mode , rather than in a unidirectional mode , taking the latter as downstream tasks of the former . ( 2 ) Naturally empowered by linguistic clues from joint learning , pre - trained language models will be more powerful for enhancing downstream tasks . Thus we propose Linguistics Informed Multi - Task BERT ( LIMIT - BERT ) , making an attempt to in- corporate linguistic knowledge into pre - training language representation models . The proposed LIMIT - BERT is implemented in terms of Multi- Task Learning ( MTL ) ( Caruana , 1993 ) which has shown useful , by alleviating overfitting to a spe- cific task , thus making the learned representations universal across tasks .\\n\\nSince universal language representations are learning by leveraging large amounts of unlabeled data which has quite different data volume com- pared with linguistics tasks dataset such as Penn Treebank ( PTB ) ¹ ( Marcus et al . , 1993 ) .\\n\\nTo alleviate such data unbalance on multi - task\\n\\nTo alleviate such data unbalance on multi - task\\n\\n' PTB is an English treebank with syntactic tree annotation which only contains 50k sentences .\\n\\nlearning , we apply semi - supervised learning ap- proach that uses a pre - trained linguistics model² to annotate large amounts of unlabeled text data and to combine with gold linguistics task dataset as our fi- nal training data . For such pre - processing , it is easy to train our LIMIT - BERT on large amounts of data with many tasks concurrently by simply summing up all the concerned losses together . Moreover , since every sentence has labeled with predicted syntax and semantics , we can furthermore improve the masked training objective by fully exploiting the known syntactic or semantic constituents dur- ing the language model training process . Unlike the previous work MT - DNN ( Liu et al . , 2019b ) which only fine - tunes BERT on GLUE tasks , our LIMIT - BERT is trained on large amounts of data in a semi - supervised way and firmly supported by explicit linguistic clues .\\n\\nWe verify the effectiveness and applicability of LIMIT - BERT on Propbank semantic parsing 3 in both span style ( CoNLL - 2005 ) ( Carreras and Màrquez , 2005 ) and dependency style , ( CoNLL- 2009 ) ( Hajič et al . , 2009 ) and Penn Treebank ( PTB ) ( Marcus et al . , 1993 ) for both constituent and de- pendency syntactic parsing . Our empirical results show that semantics and syntax can indeed ben- efit the language representation model via multi- task learning and outperforms the strong baseline Whole Word Masking BERT ( BERTWWM ) .\\n\\n2 Tasks and Datasets\\n\\nLIMIT - BERT includes five types of downstream tasks : Part - Of - Speech , constituent and dependency syntactic parsing , span and dependency semantic role labeling ( SRL ) .\\n\\nBoth span ( constituent ) and dependency are two broadly - adopted annotation styles for either seman- tics or syntax , which have been well studied and discussed from both linguistic and computational perspectives ( Chomsky , 1981 ; Li et al . , 2019 ) .\\n\\nConstituency parsing aims to build a constituency - based parse tree from a sentence that represents its syntactic structure according to a phrase structure grammar . While dependency parsing identifies syntactic relations ( such as an adjective modifying a noun ) between word pairs in a sentence . The constituent structure\\n\\n2The model may jointly predict syntax and semantics for both span and dependency annotation styles , which is from ( Zhou et al . , 2020 ) and joint learning with POS tag .\\n\\n3It is also called semantic role labeling ( SRL ) for the se- mantic parsing task over the Propbank .\\n\\nis better at disclosing phrasal continuity , while the dependency structure is better at indicating dependency relation among words .\\n\\nSemantic role labeling ( SRL ) is dedicated to rec- ognizing the predicate - argument structure of a sen- tence , such as who did what to whom , where and when , etc. For argument annotation , there are two formulizations . One is based on text spans , namely span - based SRL . The other is dependency - based SRL , which annotates the syntactic head of argu- ment rather than the entire argument span . SRL is an important method to obtain semantic informa- tion beneficial to a wide range of NLP tasks ( Zhang et al . , 2019 ; Mihaylov and Frank , 2019 ) .\\n\\nBERT is typically trained on quite large un- labeled text datasets , BooksCorpus and English Wikipedia , which have 13GB plain text , while the datasets for specific linguistics tasks are less than 100MB . Thus we employ semi - supervised learn- ing to alleviate such data unbalance on multi - task learning by using a pre - trained linguistics model to label BooksCorpus and English Wikipedia data . The pre - trained model jointly learns POS tags and the four types of structures on semantics and syn- tax , in which the latter is from the XLNet version of ( Zhou et al . , 2020 ) , giving state - of - the - art or comparable performance for the concerned four parsing tasks . During training , we set 10 % proba- bility to use gold syntactic parsing and SRL data : Penn Treebank ( PTB ) ( Marcus et al . , 1993 ) , span style SRL ( CONLL - 2005 ) ( Carreras and Màrquez , 2005 ) and dependency style SRL ( CONLL - 2009 ) ( Hajič et al . , 2009 ) .\\n\\n2.1 Linguistics - Guided Mask Strategy\\n\\nBERT applies two training objectives : Masked Language Model ( LM ) and Next Sentence Predic- tion ( NSP ) based on WordPiece embeddings ( Wu et al . , 2016 ) with a 30,000 token vocabulary . For Masked LM training objective , BERT uses training data generator to choose 15 % of the token posi- tions at random for mask replacement and predict the masked tokens4 . Since using different mask- ing strategy can improve model performance such as the Whole Word Masking5 which masks all of the tokens corresponding to a word at once , we further improve the masking strategy by exploit-\\n\\nActually , BERT applies three replacement strategies : ( 1 ) the [ MASK ] token 80 % of the time ( 2 ) random token 10 % of the time ( 3 ) the unchanged i - th token 10 % of the time . This work uses the same replacement strategies .\\n\\nShttps : //github.com/huggingface/transformers\\n\\nA0\\n\\nΑ1\\n\\nA0\\n\\nAl\\n\\nFederal Paper Board sells paper and wood products\\n\\nA2\\n\\nAl\\n\\nΑΠ\\n\\nSpan and Dependency SRL\\n\\nAl\\n\\nFederal Paper Board sells paper and wood products\\n\\nA2\\n\\nAl\\n\\nΑΠ\\n\\nSpan and Dependency SRL\\n\\nfederal paper board [ MASK ] paper and wood [ MASK ] .\\n\\n( a ) Semantic Phrase Masking .\\n\\nNP ( 1,3 )\\n\\nS ( 1,9 )\\n\\nVP ( 4,8 )\\n\\nNNP NNP NNP Federal Paper Board\\n\\nVBZ\\n\\nNP\\n\\nsells\\n\\n( 5,8 )\\n\\n4\\n\\n2\\n\\n3\\n\\n5\\n\\nNN CC paper and 6\\n\\nNN NNS wood products 7 8\\n\\nConstituent Syntactic Tree\\n\\n[ MASK ] [ MASK ] [ MASK ] sells paper and wood products .\\n\\n( b ) Syntactic Phrase Masking .\\n\\nFigure 1 : Syntactic and Semantic Phrase Masking strat- egy . In figure ( a ) the predicates sells and products are replaced by [ MASK ] while in figure ( b ) each token of constituent federal paper board also has been masked .\\n\\ning available linguistic clues , syntactic or seman- tic constituents ( phrases ) , predicted by our pre- trained linguistics model as discussed in Section 2. Thus , we apply three mask strategies at ran- dom for each sentence : Syntactic Phrase Masking , Semantic Phrase Masking , and Whole Word Mask- ing . Syntactic / Semantic Phrase Masking ( SPM ) means that all the tokens corresponding to a syntac- tic / semantic phrase are masked , as shown in Fig- ure 1. The overall masking rate and replacement strategy remain the same as BERT , we still predict each masked WordPiece token independently . In- tuitively , it makes sense that SPM is strictly more powerful than original Token Masking or Whole Word Masking , since SPM may choose and pre- dict the meaningful words or phrases such as verb predicates or noun phrases .\\n\\n3 LIMIT - BERT Model 3.1 Overview\\n\\nThe architecture of the LIMIT - BERT is shown in Figure 2. Our model includes four modules : to- ken representation , Transformer encoder , language modeling layers , task - specific layers including syn-\\n\\n6Syntactic phrases indicate the constituent subtrees while semantic phrases represent as predicate or argument in span\\n\\nSRL .\\n\\nSRL .\\n\\ntactic and semantic scorers and decoders . We take multi - task learning ( MTL ) ( Caruana , 1993 ) sharing the parameters of token representation and Trans- former encoder , while language modeling layers and the top task - specific layers have independent parameters . The training procedure is simple that we just sum up the language model loss with task- specific losses together .\\n\\n3.2 Token Representation\\n\\nFollowing BERT token representation ( Devlin et al . , 2019 ) , the first token is always the [ CLS ] token . If input X is packed by a sentence pair X1 ; X2 , we separate the two sentences with a spe- cial token [ SEP ] ( \" packed by \" means connect two sentences as BERT training ) . The Transformer en- coder maps X into a sequence of input embedding vectors , one for each token , which is a sum of the corresponding word , segment , and positional em- beddings .\\n\\nIf we apply BERT training data ( BooksCorpus and English Wikipedia ) , we use pair sentences packed to perform next sentence prediction and only take the first sentence including [ CLS ] and [ SEP ] token for later linguistics tasks . While using gold linguistics task data ( PTB , CONLL - 2005 , and CONLL - 2009 ) with 10 % probability , we only take one sentence as input that [ CLS ] and [ SEP ] are first and last tokens respectively .\\n\\nSince input sequence X is based on WordPiece token , we only take the last WordPiece vector of the word in the last layer of Transformer encoder as our sole word representation for later linguistics tasks input to keep the same length of the token and label annotations .\\n\\n3.3 Transformer Encoder\\n\\nThe Transformer encoder in our model is adapted from ( Vaswani et al . ) , which transforms the input representation vectors into a sequence of contextu- alized embedding vectors with shared representa- tion across different tasks . We use the pre - trained parameters of BERT ( Devlin et al . , 2019 ) as our encoder initialization for faster convergence .\\n\\n3.4 Language Modeling Layers\\n\\nBERT training applies masked language modeling ( MLM ) as a training objective which corrupts the input by replacing some tokens with a special token [ MASK ] and then lets the model reconstruct the original tokens . While in our LIMIT - BERT train- ing , the linguistics specific tasks and MLM train-\\n\\n[ CLS ]\\n\\n[ CLS ]\\n\\nROOT\\n\\n[ Categ < S\\n\\nHEAD sells ( 1,9 )\\n\\nPart - Of - Speech Score\\n\\n[ Categ < NP > HEAD Board ( 1,3 )\\n\\nNNP NNP NNP Federal Paper Board\\n\\n2\\n\\n[ Categ < VP > ] HEAD sells ( 4,8 )\\n\\nVBZ [ Categ NP > sells HEAD products\\n\\n4\\n\\n( 5,8 )\\n\\n[ < # >\\n\\n[ Categ aper\\n\\nHEAD\\n\\n( 5,7 )\\n\\nFederal\\n\\nFederal\\n\\nDependency Head Score\\n\\nPaper\\n\\nPaper\\n\\n[ MASK ]\\n\\n[ SEP ]\\n\\n[ MASK ]\\n\\n+\\n\\nproducts\\n\\nNNS\\n\\n|\\n\\nproducts\\n\\n⑩\\n\\nConstituent Span Score\\n\\nNN paper\\n\\nCC NN\\n\\nand wood\\n\\nLabel scorel\\n\\n\\ Joint Span Structure\\n\\n6\\n\\nBroadcast\\n\\n[ SEP ]\\n\\nPredicate score [\\n\\nSound\\n\\nfamiliar\\n\\nfamiliar\\n\\n[ SEP ]\\n\\n[ SEP ]\\n\\nArgument score\\n\\nSemantic Role Score\\n\\nTask Specific Layer\\n\\nSoftmax\\n\\nA0\\n\\nΑΓ Al\\n\\nFederal Paper Board sells paper and wood products\\n\\nA0\\n\\nSpan and Dependency SRL Decoder Layer\\n\\nMasked Token Prediction\\n\\nReplaced Token Detection\\n\\nTask - specific Loss Language Model Loss\\n\\nAl\\n\\nToken Generator Token Discriminator Representation Transformer Representation Transformer\\n\\nLanguage Modeling Layer\\n\\nFigure 2 : The framework of LIMIT - BERT .\\n\\ning take the same input ; thus the [ MASK ] tokens raise a mismatch problem that the model sees artifi- cial [ MASK ] tokens during MLM training but not when being fine - tuned and inference on linguistics tasks . Besides , due to learning bidirectional rep- resentations , MLM approaches incur a substantial computational cost increase because the network only learns from 15 % of the tokens per example and needs more training time to converge .\\n\\nRecently ( Yang et al . , 2019 ; Clark et al . , 2020 ) have made attempts to alleviate such a difficulty . The latter applies a replaced token detection task in their ELECTRA model . Instead of masking the input , ELECTRA corrupts it by replacing some input tokens with plausible alternatives sampled from a small generator network , which is close to the original input without [ MASK ] tokens .\\n\\nWe adopt the ELECTRA training approach in our LIMIT - BERT , which lets the generator G and discriminator D share the same parameters and embedding as shown in Figure 2. The generator G is identical to BERT training ( Devlin et al . , 2019 ) that predicts the masked tokens and next sentence and sums token mask loss and next sentence predict loss as JG ( ) . Then the discriminator D takes the\\n\\nIf using gold linguistics task data , we only compute the token mask loss .\\n\\nLoss Calculation\\n\\npredicted tokens by generator G8 and is trained to distinguish tokens that have been replaced by generator G which is a simple binary classification of each token with loss JD ( 0 ) . At last , we take the output vector X of discriminator D to feed the following task - specific layers and sum the loss of JG ( 0 ) and JD ( 0 ) as the final language modeling loss Jim ( 0 ) :\\n\\nJim ( 0 ) = JG ( 0 ) + \\ JD ( 0 ) ,\\n\\nwhere is set to 50 as the same as ELECTRA . 3.5 Task - specific Layers\\n\\nFirstly , we rebuild word representations from the WordPiece tokens for linguistics tasks . Then we follow ( Zhou et al . , 2020 ) to construct the task- specific layers , including scoring layer and decoder layer . The former scores three types of linguistic objectives , dependency head , syntactic constituent and semantic role . The latter is to generate the legal linguistics structures .\\n\\nWord Level Construction Suppose that X is the output of the discriminator Transformer en- coder , we pre - process the WordPiece sequence vec- tor X for linguistics specific tasks learning which For the non - masked tokens , we take the original tokens as input .\\n\\nare based on word level . We only take the first sentence X₁ including token [ CLS ] and [ SEP ] of packed sentence pair ( X1 ; X2 ) . Then we convert WordPiece sequence vector to word - level by sim- ply taking the last WordPiece token vector of the word as the representation of the whole word . Scoring Layer\\n\\nAfter the word - level construction , we calculate the POS tag , syntactic constituent , dependency head , and semantic role scores , following the train- ing way as ( Zhou et al . , 2020 ) to construct syn- tactic constituent , dependency head , and semantic role scores objective loss which are represented as J1 ( 0 ) , J2 ( 0 ) and J3 ( 0 ) respectively .\\n\\nFor POS tagging model training , we apply a one - layer feedforward network and minimize the negative log - likelihood of the gold POS tag gpi of each word , which is implemented as a cross- entropy loss :\\n\\nJ4 ( 0 ) = -log Pe ( gpi | xi ) ,\\n\\nwhere x is word vector inside X.\\n\\nUtilizing these specific task scores , we do a sum to obtain the linguistics task loss Jit ( 0 ) for training :\\n\\nJit ( 0 ) = J1 ( 0 ) + J2 ( 0 ) + J3 ( 0 ) + J4 ( 0 ) .\\n\\nAt last , our LIMIT - BERT is trained for simply min- imizing the overall loss :\\n\\nJoverall ( 0 ) = Jim ( 0 ) + Jit ( 0 ) .\\n\\nDecoder Layer For syntactic parsing , we apply the joint span CKY - style algorithm to generate constituent and dependency syntactic tree simulta- neously by following ( Zhou and Zhao , 2019 ) .\\n\\nFor span and dependency SRL , we use a sin- gle dynamic programming decoder according to the uniform semantic role score following the non- overlapping constraints : span semantic arguments for the same predicate do not overlap ( Punyakanok et al . , 2008 ) . For further details of the scoring and decoder layer , please refer to ( Zhou et al . , 2020 ) .\\n\\n4 Experiments\\n\\n4.1 Evaluation\\n\\nWe use the model of ( Zhou et al . , 2020 ) with fine- tuned uncased BERTWWM ( whole word masking ) as the baseline . For fairly compared to the baseline ⁹Our codes and the pre - trained models https://github.com/DoodleJZ/LIMIT-BERT .\\n\\n:\\n\\nBERTWWM , we also extract the language modeling layer of LIMIT - BERT and use the same model of ( Zhou et al . , 2020 ) to fine - tune . We evaluate our proposed model LIMIT - BERT and baseline model BERTWWM on CoNLL - 2009 shared task ( Hajič et al . , 2009 ) for dependency - style SRL , CONLL- 2005 shared task ( Carreras and Màrquez , 2005 ) for span - style SRL both using the Propbank con- vention ( Palmer et al . , 2005 ) , and English Penn Treebank ( PTB ) ( Marcus et al . , 1993 ) for con- stituent syntactic parsing , Stanford basic dependen- cies ( SD ) representation ( de Marneffe et al . , 2006 ) converted by the Stanford parser 10 for dependency syntactic parsing using the same model of ( Zhou et al . , 2020 ) to fine - tune . We follow standard data splitting and evaluate setting as ( Zhou et al . , 2020 ) and use end - to - end SRL setups of both span and dependency SRL . Since LIMIT - BERT involves all syntactic and semantic parsing tasks , it is possible to directly apply LIMIT - BERT to each task without fine - tuning and we also compare these results .\\n\\nIn order to evaluate the language model pre- training performance of our LIMIT - BERT , we also evaluate LIMIT - BERT on two widely - used datasets , The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al . , 2018 ) which is a collection of nine NLU tasks and Stan- ford Natural Language Inference ( SNLI ) ( Bowman et al . , 2015 ) to show the superiority .\\n\\n4.2 Implementation Details\\n\\nOur implementation of LIMIT - BERT is based on the PyTorch implementation of BERT¹¹ . We use a learning rate of 3e - 5 and a batch size of 32 with 1 million training steps . The optimizer and other training settings are the same as BERT ( Devlin et al . , 2019 ) . For task - specific layers including syntactic and semantic scorers and decoders , we set the same hyperparameters settings as ( Zhou et al . , 2020 ) . LIMIT - BERT model is trained on 32 NVIDIA GeForce GTX 1080Ti GPUs .\\n\\n4.3 Main Results\\n\\nSyntactic Parsing Results As shown in Table 1 and 2 , LIMIT - BERT without fine - tuning obtains 95.84 F1 score of constituent parsing and 97.14 % UAS and 95.44 % LAS of dependency parsing . Compared with baseline BERTW LIMIT - BERT\\n\\nWWM ,\\n\\n10http : //nlp.stanford.edu/software/lex-parser.html \" https://github.com/huggingface/pytorch-pretrained- BERT . We use Whole Word Masking BERT parameters as our Transformer encoder initialization .\\n\\nDozat and Manning ( 2017 )\\n\\nMa et al . ( 2018 )\\n\\nJi et al . ( 2019 )\\n\\nLiu et al . ( 2019a )\\n\\nFernández - González and Gómez - Rodríguez ( 2019 )\\n\\n96.09 95.03\\n\\nZhou and Zhao ( 2019 ) ( BERT )\\n\\nZhou et al . ( 2020 ) ( BERT )\\n\\nZhou et al . ( 2020 ) ( XLNet )\\n\\nBaseline ( BERTWWM )\\n\\nOur LIMIT - BERT\\n\\nOur LIMIT - BERT +\\n\\nUAS LAS\\n\\nModel\\n\\nDev\\n\\nTest\\n\\n95.74 94.08\\n\\n95.87 94.19\\n\\nDRCN ( Kim et al . , 2018 )\\n\\n90.1\\n\\n95.97 94.31\\n\\nSJRC ( Zhang et al . , 2019 )\\n\\n91.3\\n\\n96.04 94.43\\n\\nMT - DNN ( Liu et al . , 2019b )\\n\\n92.2\\n\\n91.6\\n\\n97.00 95.43\\n\\nSemBERT ( Zhang et al . , 2020a )\\n\\n91.9\\n\\n92.2\\n\\n91.6\\n\\n97.00 95.43\\n\\nSemBERT ( Zhang et al . , 2020a )\\n\\n91.9\\n\\n96.90 95.32\\n\\n97.23 95.65\\n\\nBaseline ( BERTWWM )\\n\\n91.7\\n\\n91.4\\n\\n96.89 95.22\\n\\nLIMIT - BERT\\n\\n92.3\\n\\n91.7\\n\\n96.94 95.30 97.14 95.44\\n\\nTable 1 : Dependency syntactic parsing on PTB , no finetuning result is marked by t .\\n\\nGaddy et al . ( 2018 )\\n\\nKitaev and Klein ( 2018 ) ( ELMO )\\n\\nKitaev et al . ( 2019 ) ( BERT )\\n\\nZhou and Zhao ( 2019 ) ( BERT ) Zhou et al . ( 2020 ) ( BERT )\\n\\nZhou et al . ( 2020 ) ( XLNet ) Baseline ( BERTWWM ) Our LIMIT - BERT\\n\\nOur LIMIT - BERT +\\n\\nLR LP F1 91.76 92.41 92.08 94.85 95.40 95.13 95.46 95.73 95.59 95.70 95.98 95.84 95.39 95.64 95.52 96.10 96.26 96.18 95.59 95.86 95.72 95.67 95.92 95.80 95.72 95.96 95.84\\n\\nTable 2 : Constituent syntactic parsing on PTB , no fine- tuning result is marked by f .\\n\\nWSJ\\n\\nBrown\\n\\nSystem\\n\\nP\\n\\nR\\n\\nF1\\n\\nP\\n\\nR\\n\\nF1\\n\\nEnd - to - end Span SRL\\n\\nHe et al . ( 2018a )\\n\\nHe et al . ( 2018a ) ( ELMo ) Li et al . ( 2019 ) ( ELMO ) Strubell et al . ( 2018 ) ( ELMO ) Zhou et al . ( 2020 ) ( BERT ) Zhou et al . ( 2020 ) ( XLNet )\\n\\nBaseline ( BERTWWM ) Ours LIMIT - BERT\\n\\nOurs LIMIT - BERT +\\n\\nEnd - to - end Dependency SRL\\n\\nLi et al . ( 2019 )\\n\\nHe et al . ( 2018b )\\n\\nCai et al . ( 2018 )\\n\\nLi et al . ( 2019 ) ( ELMO )\\n\\nZhou et al . ( 2020 ) ( BERT ) Zhou et al . ( 2020 ) ( XLNet )\\n\\nBaseline ( BERTWWM ) Ours LIMIT - BERT Ours LIMIT - BERT +\\n\\n81.2 83.9 82.5 69.7 71.9 70.8 84.8 87.2 86.0 73.9 78.4 76.1 85.2 87.5 86.3 74.7 78.1 76.4 87.13 86.67 86.90 79.02 77.49 78.25 86.46 88.23 87.34 77.26 80.20 78.70 87.48 89.51 88.48 80.46 84.15 82.26 86.48 88.59 87.52 79.4 82.68 81.01 86.62 89.12 87.85 79.58 83.05 81.28 87.16 88.51 87.83 79.20 80.29 79.74\\n\\n85.1\\n\\n83.9 82.7 83.3\\n\\n84.7 85.2 85.0\\n\\n73.8\\n\\n72.5 74.2\\n\\n85.1\\n\\n83.9 82.7 83.3\\n\\n84.7 85.2 85.0\\n\\n73.8\\n\\n72.5 74.2\\n\\n84.5 86.1 85.3 74.6 86.77 89.14 87.94 79.71 82.40 81.03 86.35 90.16 88.21 80.90 85.38 83.08 85.13 89.21 87.12 79.05 83.95 81.43 85.84 90.01 87.87 79.50 84.85 82.09 85.73 89.34 87.50 79.60 82.81 81.17\\n\\nTable 3 : Span SRL and dependency SRL results on on CONLL - 2005 and CoNLL - 2009 test sets in end - to - end mode , no finetuning result is marked by † .\\n\\noutperforms the baseline model both of fine - tuning or not . Particularly , LIMIT - BERT without fine- tuning exceeds more than 0.2 in UAS of depen- dency and 0.1 F1 of constituent syntactic pars- ing which are considerable improvements on such strong baselines .\\n\\nSemantic Parsing Results Table 3 shows re- sults on CoNLL - 2005 , CONLL - 2009 in - domain ( WSJ ) and out - domain ( Brown ) test sets and com- pares our LIMIT - BERT with previous published\\n\\nTable 4 : Leaderboards of SNLI dataset . Both our LIMIT - BERT and BERTWWM are single models .\\n\\nstate - of - the - art models in end - to - end mode . The upper part of the table presents results from span SRL while the lower part shows results of depen- dency SRL . Compared with baseline , LIMIT - BERT with fine - tuning outperforms BERTWWM on all four SRL datasets , exceeding more than 0.3 in F1 of in - domain span SRL and 0.7 F1 of dependency SRL , which demonstrate that LIMIT - BERT can furthermore improve SRL performance even over strong baselines .\\n\\nThe results of syntactic and semantic parsing empirically illustrate that incorporating linguis- tic knowledge into pre - trained language model by multi - task and semi - supervised learning can signif- icantly enhance downstream tasks . SNLI Results\\n\\nTable 4 includes the best results\\n\\nreported in the leaderboards 12 of SNLI . We see that LIMIT - BERT outperforms the strong baseline model BERTwwм in 0.3 F1 score on the SNLI benchmark .\\n\\nGLUE Results We fine - tuned LIMIT - BERT for each GLUE task on task - specific data . The dev results in Table 5 show that LIMIT - BERT outper- forms the strong baseline model and achieves re- markable results compared to other state - of - the - art models in literature .\\n\\n4.4 Discussions Ablation Study\\n\\nLIMIT - BERT contains three key components : Multi - Task learning , ELECTRA training approach , and Syntactic / Semantic Phrase Masking ( SPM ) . To evaluate the contribution of each component in LIMIT - BERT , we remove each component from the model for training and then fine - tune on downstream NLU tasks and linguistics tasks for evaluation . In consideration of compu- tational cost , we apply BERT base as the start of training and only use one - tenth of the BERT train- ing corpus . We employ the same training setting\\n\\n12 https://nlp.stanford.edu/projects/\\n\\nsnli /\\n\\nModel\\n\\nCOLA SST - 2\\n\\n( mc ) ( acc )\\n\\nMRPC STS - B QQP ( F1 / acc ) ( pc / sc ) ( acc / F1 ) Dev set results for Comparison\\n\\nMNLI QNLI RTE Score m / mm ( acc ) ( acc ) ( acc )\\n\\nBERT\\n\\n60.6 93.2\\n\\nMT - DNN\\n\\nELECTRA\\n\\nBaseline ( BERTWWM )\\n\\n63.5 94.3 69.3 96.0 63.6 93.6\\n\\nLIMIT - BERT\\n\\n64.0 94.0\\n\\n- / 88.0 - / 90.0 91.3 / - 91.0 / 87.5 90.7 / 90.6 91.9 / 89.2 - / 90.6 - / 92.1 92.4 / - 90.8 / 87.0 90.5 / 90.2 91.7 / 88.8 94.0 / 91.7 91.5 / 91.3 91.6 / 88.6 87.4 / 87.3 Test set results for models with standard single - task finetuning\\n\\n93.5\\n\\n- / 86.6 92.3 70.4 84.0 87.1 / 86.7 92.9 83.4 - / 90.5 94.5 86.8 89.0 87.4 / 87.2 93.9 77.3 85.6 85.2 87.3\\n\\nBILSTM + ELMO + Attn 36.0 90.4 84.9 / 77.9 75.1 / 73.3 84.7 / 64.8 76.4 / 76.1\\n\\n56.8\\n\\n70.5\\n\\nBERT\\n\\nMT - DNN\\n\\nSemBERT\\n\\nELECTRA\\n\\n71.7\\n\\nLIMIT - BERT\\n\\n62.5\\n\\n94.5\\n\\n60.5 94.9 89.3 / 85.4 87.6 / 86.5 89.3 / 72.1 62.5 95.6 91.1 / 88.2 89.5 / 88.8 89.6 / 72.7 62.3 94.6 91.2 / 88.3 87.8 / 86.7 89.8 / 72.8 97.1 93.1 / 90.7 92.9 / 92.5 90.8 / 75.6 90.9 / 88.0 90.3 / 89.7 89.5 / 71.9\\n\\n86.7 / 85.9 92.7 70.1 86.7 / 86.0 93.1 81.4 87.6 / 86.3 94.6 84.5 82.9 91.3 / 90.8 95.8 89.8 89.35 87.1 / 86.2 94.0 83.0 83.3\\n\\n80.5\\n\\n82.7\\n\\nTable 5 : Comparison of GLUE dev and test sets . Our model is in boldface . MT - DNN dev results are from ( Liu et al . , 2019b ) and other dev results are from ( Clark et al . , 2020 ) .\\n\\nSystem\\n\\nDev\\n\\nLIMIT - BERT\\n\\n82.6\\n\\nGLUE SNLI SNLI Dev Test 90.6 91.0\\n\\nSEM span SEM dep SYNcon\\n\\nSYN dep\\n\\nSystem\\n\\nF1\\n\\nF1\\n\\nw / o Multi - Task 82.9 w / o ELECTRA 81.3 w / o SPM\\n\\n90.5 90.7\\n\\nBaseline LIMIT - BERT 87.13 LIMIT - BERT + 87.04\\n\\n86.55\\n\\n86.10\\n\\n86.77\\n\\n86.38\\n\\nF1 95.52 96.54 94.71 95.55 96.54 94.74 95.72 96.82 94.82\\n\\nUAS LAS\\n\\n90.4 90.5\\n\\n80.2\\n\\n90.6 90.8\\n\\nTable 6 :\\n\\nGLUE and SNLI .\\n\\nAblation study of LIMIT - BERT ( base ) on\\n\\nSEM span SEM dep\\n\\nSystem\\n\\nF1\\n\\nF1\\n\\nLIMIT - BERT\\n\\n86.25\\n\\n85.74\\n\\nSYN con F1 95.34\\n\\nw / o Multi - Task\\n\\n85.60\\n\\n85.24\\n\\nw / o ELECTRA\\n\\n86.20\\n\\n85.71\\n\\nw / o SPM\\n\\n86.21\\n\\n85.72\\n\\nSYN dep UAS LAS 96.59 94.71 95.16 96.38 94.38 95.32 96.59 94.70 95.44 96.64 94.70\\n\\nTable 7 : Ablation study of LIMIT - BERT ( base ) on lin- guistics tasks .\\n\\nfor each ablation model : 200k training steps , le - 5 learning rate and 32 batch size . After language model training , we extract the layers of BERT base and fine - tune on downstream tasks for evaluation .\\n\\nThe ablation study is conducted on NLU tasks and linguistics tasks shown in Table 6 and 7 respec- tively . For NLU tasks , GLUE and SNLI results both decrease in w / o ELECTRA and w / o SPM set- ting showing the effectiveness of the ELECTRA training approach and SPM for NLU tasks . For linguistics tasks , w / o Multi - Task setting hurts the performance obviously from LIMIT - BERT both of semantic and syntactic parsing , which shows the effectiveness of multi - tasks learning for linguistics tasks . Besides , the ELECTRA training approach\\n\\nTable 8 : Fine - tuning effect analysis on English dev sets , no finetuning result is marked by † .\\n\\nand SPM also can improve performance when fine- tuning on linguistics tasks .\\n\\nComparing the results in Tables 6 and 7 , ELEC- TRA training approach and SPM are more effec- tive for NLU tasks while multi - tasks learning can improve the linguistics tasks performance signifi- cantly . The possible explanation is that multi - tasks learning enables LIMIT - BERT to ' remember ' the linguistics information and thus lead to better per- formance in downstream linguistics tasks . Fine - tuning Effect We examine the fine - tuning effect of LIMIT - BERT on linguistics tasks . The results in Table 8 show that LIMIT - BERT with or without finetuning still outperforms BERTWWM baseline consistently among all tasks . In such a case , fine - tuning is necessary to boost the seman- tic parsing performance while no - fine - tuning per- forms better on syntactic parsing . As shown in Ta- ble 8 , the accuracy improves 0.1 F1 and 0.4 F1 of span SRL and dependency SRL after fine - tuning re- spectively but no - fine - tuning performs better nearly 0.2 F1 of syntactic parsing . The possible explana- tion is that no - fine - tuning LIMIT - BERT use semi- supervised training data which contains much more long sentence samples and benefits syntactic pars- ing more .\\n\\n97.59\\n\\nModel\\n\\nTest\\n\\nYasunaga et al . ( 2018 )\\n\\nAkbik et al . ( 2018 )\\n\\nBohnet et al . ( 2018 )\\n\\nLIMIT - BERT\\n\\n97.85\\n\\n97.96\\n\\n97.71\\n\\nTable 9 : POS tagging accuracy on the WSJ test set , with other top - performing systems .\\n\\nF1 of Constituent Parsing\\n\\nF1 of Span SRL\\n\\n100\\n\\n90\\n\\n81\\n\\n85\\n\\n0-9\\n\\n78\\n\\n0-9\\n\\nBaseline Constituent\\n\\nLimitBert Constituent Baseline Dependency LimitBert Dependency\\n\\n100\\n\\ntotal\\n\\n10-19 20-29 30-39 40-49 50-59 60-69\\n\\nSyntactic Parsing of Different Sentences Length\\n\\nBaseline Span LimitBert Span\\n\\nBaseline Dependency LimitBert Dependency\\n\\n60-69 total\\n\\nBaseline Span LimitBert Span\\n\\nBaseline Dependency LimitBert Dependency\\n\\n60-69 total\\n\\n10-19 20-29 30-39 40-49 50-59 SRL of Different Sentences Length\\n\\n95\\n\\n90\\n\\n85\\n\\n80\\n\\n75\\n\\n90\\n\\n78\\n\\nUAS of Dependency Parsing\\n\\nF1 of Dependency SRL\\n\\nFigure 3 : The performance of baseline model and LIMIT - BERT while varying sentence length of four lin- guistics tasks on the English dev set .\\n\\nPart - Of - Speech Performance\\n\\nTable 9 lists the\\n\\nresults of POS tagging on WSJ test set showing that our LIMIT - BERT achieves competitive results compared with other state - of - the - art models . Note that we only apply simple one - layer decoder with- out those complicated ones such as conditional random field ( CRF ) ( Lafferty et al . , 2001 ) as the POS tagging task is not the main concern of our model .\\n\\nSentences Length Performance\\n\\nThe perfor-\\n\\nmance of baseline model and LIMIT - BERT while varying the sentence length of four linguistics tasks on the English dev set is shown in Figure 3. The statistics show that our LIMIT - BERT outperforms the baseline model of over all sentence lengths . For different sentence lengths , LIMIT - BERT outper- forms much better than baseline model on long sen- tence ( larger than 50 ) of both syntactic and seman- tic parsing . The possible explanation is that LIMIT-\\n\\nBERT uses semi - supervised training data , which contains much more long sentence samples and benefits parsing performance on long sentences .\\n\\n5 Related Work\\n\\nLinguistics Inspired NLP Tasks With the im- pressive success of deep neural networks in various NLP tasks ( Chen and Manning , 2014 ; Dozat and Manning , 2017 ; Ma et al . , 2018 ; Strubell et al . , 2018 ; Luo and Zhao , 2020 ; Li et al . , 2020 ; He et al . , 2019 ; Luo et al . , 2020 ; Zhang et al . , 2018a ; Li et al . , 2018a ; Zhang et al . , 2018b ) , syntactic parsing and semantic role labeling have been well developed with neural network and achieve very\\n\\nhigh performance ( Chen and Manning , 2014 ; Dozat and Manning , 2017 ; Ma et al . , 2018 ; Kitaev and Klein , 2018 ; Zhou and Zhao , 2019 ) . Semantic role labeling is deeply related to syntactic structure and a number of works try to incorporate syntac- tic information in semantic role labeling models by different methods such as concatenation of lex- icalized embedding ( He et al . , 2018b ) , usage of syntactic GCN ( Li et al . , 2018b ) and multi - task learning ( Strubell et al . , 2018 ; Zhou et al . , 2020 ) . Besides semantic role labeling and syntactic pars- ing are two key tasks of semantics and syntax so that they are included into our linguistics tasks for multi - task learning .\\n\\nIn addition , both span and dependency are pop- ularly adopted annotation styles for both seman- tics and syntax and some work on jointly learn- ing of semantic and syntactic ( Henderson et al . , 2013 ; Lluís et al . , 2013 ; Swayamdipta et al . , 2016 )\\n\\n.\\n\\nResearchers are interested in two styles of SRL models that may benefit from each other rather than their separated development , which has been roughly discussed in ( Johansson and Nugues , 2008 ) . On the other hand , researchers have discussed how to encode lexical dependencies in phrase structures , like lexicalized tree adjoining grammar ( LTAG ) ( Schabes et al . , 1988 ) , Combina- tory Categorial Grammar ( CCG ) ( Steedman , 2000 ) and head - driven phrase structure grammar ( HPSG ) ( Pollard and Sag , 1994 ) which is a constraint- based highly lexicalized non - derivational genera- tive grammar framework . To absorb both strengths of span and dependency structure , we apply both span ( constituent ) and dependency representations of semantic role labeling and syntactic parsing . Thus , it is a natural idea to study the relationship be- tween constituent and dependency structures , and\\n\\nthe joint learning of constituent and dependency syntactic parsing ( Klein and Manning , 2004 ; Char- niak and Johnson , 2005 ; Farkas et al . , 2011 ; Green and Žabokrtský , 2012 ; Ren et al . , 2013 ; Xu et al . , 2014 ; Yoshikawa et al . , 2017 ) .\\n\\nPre - trained Language Modeling\\n\\nRecently ,\\n\\ndeep contextual language model has been shown effective for learning universal language represen- tations by leveraging large amounts of unlabeled data , achieving various state - of - the - art results in a series of NLU benchmarks . Some prominent examples are Embedding from Language models ( ELMO ) ( Peters et al . , 2018 ) , Bidirectional Encoder Representations from Transformers ( BERT ) ( De- vlin et al . , 2019 ) and Generalized Autoregressive Pretraining ( XLNet ) ( Yang et al . , 2019 ) .\\n\\nMany latest works make attempts to modify the language model based on BERT such as ELEC- TRA ( Clark et al . , 2020 ) and MT - DNN ( Liu et al . , 2019b ) . ELECTRA focuses on the [ MASK ] tokens mismatch problem and thus combines the idea of Generative Adversarial Networks GANS ( Good- fellow et al . ) . MT - DNN applies multi - task learn- ing to language model pre - training and achieves new state - of - the - art results on GLUE benchmark . Besides , ( Gururangan et al . , 2020 ) finds that mul- tiphase adaptive pretraining offers large gains in task performance which is similar with our semi- supervised learning strategy .\\n\\n6 Conclusions\\n\\n6 Conclusions\\n\\nIn this work , we present LIMIT - BERT which ap- plies multi - task learning with multiple linguistic tasks by semi - supervised learning . We use five key syntax and semantics tasks : Part - Of - Speech ( POS ) tags , constituent and dependency syntactic parsing , span and dependency semantic role label- ing ( SRL ) . and further improve the masking strat- egy of BERT training by effectively exploiting the available syntactic and semantic clues for language model training . The experiments show that LIMIT- BERT outperforms the strong baseline BERTwWM on four benchmark parsing treebanks and two NLU tasks . The results of GLUE and SNLI empirically illustrate that incorporating linguistic knowledge into pre - training language BERT by multi - task and semi - supervised learning can also enhance down- stream tasks . There are many future areas to ex- plore to improve LIMIT - BERT , including a deeper understanding of model structure sharing in MTL , a more effective training method that leverages relat-\\n\\nedness among multiple tasks , for both fine - tuning and pre - training , and ways of incorporating the linguistic structure of text in a more explicit and controllable manner .\\n\\n
'''

collection = client.collections.get("pdf")
new = collection.data.insert(
    properties={
        "pdf_id" : "ea1cc84c-fd0d-4c5e-9e14-bf5fad2fd042",
        "full_text" : texts
    }
)

print(new)

